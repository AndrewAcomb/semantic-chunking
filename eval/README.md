# Evals
This is a simple python microservice that evaluates two different semantic search arrays' performance over a simple search query

Evaluation is based on idea relevance


# Running locally
- Cd into the /eval directory
- Create a python environment
    `python3 -m venv .venv`
    I call it `.venv` folder which is gitignored
- Activate the environment
    `source .venv/bin/activate`